{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de59ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TASK 1\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Define the layers of the CNN\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1) \n",
    "        # Padding is set to 1 to maintain the same spatial dimensions after convolution\n",
    "        # Kernel size is 3x3, stride is 1 to move one pixel at a time\n",
    "        # The first layer takes a single-channel input (grayscale image) and outputs 32 channels\n",
    "        # The output size after this layer will be (32, 28, 28) for a 28x28 input image\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Max pooling with a 2x2 kernel and stride of 2 reduces the spatial dimensions by half\n",
    "        # The output size after this layer will be (32, 14, 14)\n",
    "        \n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=2)\n",
    "        # The second layer takes 32 channels as input and outputs 64 channels\n",
    "        # Padding is set to 2 to maintain the same spatial dimensions after convolution\n",
    "        # The kernel size is 3x3, stride is 1 to move one pixel at a time\n",
    "        # The output size after this layer will be (64, 14, 14)\n",
    "\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Max pooling with a 2x2 kernel and stride of 2 reduces the spatial dimensions by half\n",
    "        # The output size after this layer will be (64, 7, 7)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        # The third layer takes 64 channels as input and outputs 64 channels\n",
    "        # Padding is set to 1 to maintain the same spatial dimensions after convolution\n",
    "        # The kernel size is 3x3, stride is 1 to move one pixel at a time\n",
    "        # The output size after this layer will be (64, 7, 7)\n",
    "\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Max pooling with a 2x2 kernel and stride of 2 reduces the spatial dimensions by half\n",
    "        # The output size after this layer will be (64, 3, 3)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Flatten the output from the convolutional layers to feed into the fully connected layers\n",
    "        # The output size after flattening will be (64 * 3 * 3)\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 256)\n",
    "        # The first fully connected layer takes the flattened output and outputs 256 features\n",
    "        # The input size is 64 * 4 * 4 = 1024 (after flattening)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        # The second fully connected layer takes 256 features as input and outputs 128 features\n",
    "        # The input size is 256\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "        # The third fully connected layer takes 128 features as input and outputs 10 features (for 10 classes)\n",
    "        # The input size is 128\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x))) # Apply the first convolutional layer and max pooling\n",
    "        x = self.pool2(F.relu(self.conv2(x))) # Apply the second convolutional layer and max pooling\n",
    "        x = self.pool3(F.relu(self.conv3(x))) # Apply the third convolutional layer and max pooling\n",
    "        x = self.flatten(x)                   # Flatten the output\n",
    "        x = F.relu(self.fc1(x))               # Apply the first fully connected layer\n",
    "        x = F.relu(self.fc2(x))               # Apply the second fully connected layer\n",
    "        x = self.fc3(x)                       # No softmax here because CrossEntropyLoss expects raw logits\n",
    "        return x                              # The output is the raw logits for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70fddc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "class FashionMNISTLoader:\n",
    "    def __init__(self, batch_size=64, val_split=0.0, data_dir='./data'):\n",
    "        self.batch_size = batch_size\n",
    "        self.val_split = val_split\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "\n",
    "    def load_datasets(self):\n",
    "        train_dataset = datasets.FashionMNIST(root=self.data_dir, train=True, download=True, transform=self.transform)\n",
    "        test_dataset = datasets.FashionMNIST(root=self.data_dir, train=False, download=True, transform=self.transform)\n",
    "\n",
    "        if self.val_split > 0:\n",
    "            val_size = int(self.val_split * len(train_dataset))\n",
    "            train_size = len(train_dataset) - val_size\n",
    "            train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "            self.val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        else:\n",
    "            self.val_loader = None\n",
    "\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        self.test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def get_loaders(self):\n",
    "        self.load_datasets()\n",
    "        return self.train_loader, self.val_loader, self.test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95ad2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n$ python Training-Testing.py \\nCNN(\\n  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\\n  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\\n  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\\n  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\\n  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\\n  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\\n  (flatten): Flatten(start_dim=1, end_dim=-1)\\n  (fc1): Linear(in_features=1024, out_features=256, bias=True)\\n  (fc2): Linear(in_features=256, out_features=128, bias=True)\\n  (fc3): Linear(in_features=128, out_features=10, bias=True)\\n)\\n\\nTraining with Validation Split: 0%\\n100.0%\\n100.0%\\n100.0%\\n100.0%\\nEpoch 1/10, Loss: 1.843058825873617\\nEpoch 2/10, Loss: 0.7006924451350658\\nEpoch 3/10, Loss: 0.5781377986359444\\nEpoch 4/10, Loss: 0.5174538178611666\\nEpoch 5/10, Loss: 0.4754209449328085\\nEpoch 6/10, Loss: 0.4418931762133834\\nEpoch 7/10, Loss: 0.41398337977463756\\nEpoch 8/10, Loss: 0.3898790697299087\\nEpoch 9/10, Loss: 0.3720219781292654\\nEpoch 10/10, Loss: 0.3550133986164258\\nTest Accuracy with 0% val split: 0.8582\\n\\nTraining with Validation Split: 10%\\nEpoch 1/10, Loss: 2.20172238152174\\nEpoch 2/10, Loss: 0.8433410273909003\\nEpoch 3/10, Loss: 0.6164166372604845\\nEpoch 4/10, Loss: 0.5363222860258903\\nEpoch 5/10, Loss: 0.48469826490816914\\nEpoch 6/10, Loss: 0.4481734139977191\\nEpoch 7/10, Loss: 0.4169324422299297\\nEpoch 8/10, Loss: 0.39204447418098204\\nEpoch 9/10, Loss: 0.3735168946474367\\nEpoch 10/10, Loss: 0.3570586396866783\\nTest Accuracy with 10% val split: 0.8656\\n\\nTraining with Validation Split: 20%\\nEpoch 1/10, Loss: 2.164117626508077\\nEpoch 2/10, Loss: 0.8640597453514735\\nEpoch 3/10, Loss: 0.6543839203516643\\nEpoch 4/10, Loss: 0.5852551572720209\\nEpoch 5/10, Loss: 0.5355785616238912\\nEpoch 6/10, Loss: 0.5007881287932396\\nEpoch 7/10, Loss: 0.474975066781044\\nEpoch 8/10, Loss: 0.4463097376227379\\nEpoch 9/10, Loss: 0.42608228588104247\\nEpoch 10/10, Loss: 0.4053118579387665\\nTest Accuracy with 20% val split: 0.8402\\n\\nTraining with Validation Split: 30%\\nEpoch 1/10, Loss: 2.2903281844007006\\nEpoch 2/10, Loss: 1.4476598109284493\\nEpoch 3/10, Loss: 0.704328636464463\\nEpoch 4/10, Loss: 0.6003620566479873\\nEpoch 5/10, Loss: 0.5409784692153902\\nEpoch 6/10, Loss: 0.5024089674653891\\nEpoch 7/10, Loss: 0.4675851472436565\\nEpoch 8/10, Loss: 0.4409834634586375\\nEpoch 9/10, Loss: 0.41987262625400334\\nEpoch 10/10, Loss: 0.3998038669654042\\nTest Accuracy with 30% val split: 0.7280\\n\\nTraining with Validation Split: 40%\\nEpoch 1/10, Loss: 2.2726757306188716\\nEpoch 2/10, Loss: 1.1542662258258298\\nEpoch 3/10, Loss: 0.7294745041358535\\nEpoch 4/10, Loss: 0.6414141761896666\\nEpoch 5/10, Loss: 0.5898961994406596\\nEpoch 6/10, Loss: 0.5551875661363822\\nEpoch 7/10, Loss: 0.5260721616063194\\nEpoch 8/10, Loss: 0.501393575598461\\nEpoch 9/10, Loss: 0.4792937152815543\\nEpoch 10/10, Loss: 0.45967558455086094\\nTest Accuracy with 40% val split: 0.7088\\n\\n=== Summary of Results ===\\nVal Split: 0% | Test Accuracy: 0.8582\\nVal Split: 10% | Test Accuracy: 0.8656\\nVal Split: 20% | Test Accuracy: 0.8402\\nVal Split: 30% | Test Accuracy: 0.7280\\nVal Split: 40% | Test Accuracy: 0.7088\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score  # Importing roc_auc_score for AUC calculation\n",
    "import torch.nn.functional as F # Importing functional module for softmax\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, test_loader, val_loader=None, optimizer=None, loss_fn=None, device='cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer or torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "        self.loss_fn = loss_fn or nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    def train(self, epochs=10):\n",
    "        \"\"\" Train the model on the training set.\n",
    "        Args:\n",
    "            epochs (int): Number of epochs to train the model.\n",
    "        \"\"\"\n",
    "\n",
    "        self.model.train() # Set the model to training mode\n",
    "\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0 # Initialize running loss for each epoch\n",
    "\n",
    "            for batch_idx, (inputs, labels) in enumerate(self.train_loader):\n",
    "                inputs = inputs.to(self.device) # Move data to the device (GPU or CPU)\n",
    "                labels = labels.to(self.device) # Move target to the device (GPU or CPU)\n",
    "\n",
    "                self.optimizer.zero_grad() # Zero the gradients before the backward pass\n",
    "\n",
    "                outputs = self.model(inputs) # Forward pass through the model\n",
    "\n",
    "                loss = self.loss_fn(outputs, labels) # Compute the loss\n",
    "\n",
    "                loss.backward() # Backward pass to compute gradients\n",
    "\n",
    "                self.optimizer.step() # Update the model parameters\n",
    "\n",
    "                running_loss += loss.item() # Accumulate the loss\n",
    "\n",
    "            avg_loss = running_loss / (batch_idx + 1) # Average loss for the epoch\n",
    "\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(self.train_loader)}') \n",
    "            # Print the average loss for the epoch\n",
    "\n",
    "\n",
    "    def evaluate(self, loader=None):\n",
    "        \"\"\" Evaluate the model on the test set or validation set.\n",
    "        Args:\n",
    "            loader (DataLoader): DataLoader for the test or validation set. If None, uses the test_loader.\n",
    "        Returns:\n",
    "            float: Accuracy of the model on the test set.\n",
    "        \"\"\"\n",
    "        self.model.eval() # Set the model to evaluation mode\n",
    "\n",
    "        loader = loader or self.test_loader # Use the test_loader if no loader is provided\n",
    "\n",
    "        y_true, y_pred = [], [] # Initialize lists to store true and predicted labels\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device) # Move data to the device (GPU or CPU)\n",
    "\n",
    "                outputs = self.model(images) # Forward pass through the model\n",
    "\n",
    "                preds = outputs.argmax(dim=1) # Get the predicted labels\n",
    "\n",
    "                y_true.extend(labels.cpu().numpy()) # Move labels to CPU and convert to numpy array\n",
    "\n",
    "                y_pred.extend(preds.cpu().numpy()) # Move predictions to CPU and convert to numpy array\n",
    "\n",
    "        accuracy = accuracy_score(y_true, y_pred) # Compute accuracy\n",
    "\n",
    "        return accuracy\n",
    "    \n",
    "    def evaluate_auc(self, loader= None, pos_class= None):\n",
    "        self.model.eval() # Set the model to evaluation mode\n",
    "        loader = loader or self.test_loader # Use the test_loader if no loader is provided\n",
    "        pos_class = pos_class or 1 # Default positive class is 1\n",
    "        y_true, y_scores = [], []\n",
    "\n",
    "        # Iterate through the data loader\n",
    "        with torch.no_grad(): \n",
    "            for images, labels in loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = F.softmax(self.model(images), dim=1) # Get the softmax probabilities\n",
    "                y_true.extend((labels == pos_class).cpu().numpy()) # Convert labels to binary (1 for positive class, 0 for negative class)\n",
    "                y_scores.extend(outputs[:, pos_class].cpu().numpy()) # Get the scores for the positive class\n",
    "\n",
    "        auc = roc_auc_score(y_true, y_scores) # Compute AUC score\n",
    "\n",
    "        return auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f39c316",
   "metadata": {},
   "source": [
    "CNN(\n",
    "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
    "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
    "  (fc1): Linear(in_features=1024, out_features=256, bias=True)\n",
    "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
    "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216879fc",
   "metadata": {},
   "source": [
    "# Task 1: CNN Architecture Implementation\n",
    "\n",
    "The CNN model was implemented in PyTorch as specified:\n",
    "\n",
    "- **Input:** 28x28 grayscale image\n",
    "- **Conv1:** 32 filters, 3×3, stride 1, padding 1\n",
    "- **MaxPool:** 2×2, stride 2\n",
    "- **Conv2:** 64 filters, 3×3, stride 1, padding 2\n",
    "- **MaxPool:** 2×2, stride 2\n",
    "- **Conv3:** 64 filters, 3×3, stride 1, padding 1\n",
    "- **MaxPool:** 2×2, stride 2\n",
    "- **Fully Connected Layers:** 256 → 128 → 10\n",
    "- **Output:** Logits (softmax will be applied during evaluation)\n",
    "\n",
    "The model summary confirms the layer configuration and output shapes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d3e761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CNN import CNN\n",
    "from FashionMNISTLoader import FashionMNISTLoader\n",
    "from TrainValTestSplit import Trainer\n",
    "import torch\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "# Task 1\n",
    "model = CNN()\n",
    "print(f\"Model Architecture: {model}\") # Log the model architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf61081e",
   "metadata": {},
   "source": [
    "# Task 2: Training and Testing with Varying Validation Splits\n",
    "\n",
    "We trained the CNN model on FashionMNIST using the following validation split percentages: 0%, 10%, 20%, 30%, 40%.\n",
    "\n",
    "All experiments used:\n",
    "- 10 training epochs\n",
    "- Batch size: 64\n",
    "- Optimizer: SGD\n",
    "- Learning rate: 0.1\n",
    "- Loss function: CrossEntropyLoss\n",
    "\n",
    "**Findings:**\n",
    "- Without a validation set (0%), performance was decent but lacked early stopping or tuning.\n",
    "- With increasing validation size, test accuracy first improved, then dropped.\n",
    "- Larger validation splits reduce training data, potentially underfitting the model.\n",
    "\n",
    "| Validation Split | Test Accuracy |\n",
    "|------------------|---------------|\n",
    "| 0%               | 0.8562        |\n",
    "| 10%              | 0.8591        |\n",
    "| 20%              | 0.8589        |\n",
    "| 30%              | 0.8315        |\n",
    "| 40%              | 0.8175        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc89a66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2\n",
    "print(\"=== Running Task 2 Experiments ===\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Set device to GPU if available, else CPU\n",
    "splits = [0.0, 0.1, 0.2, 0.3, 0.4] # Validation splits to test\n",
    "results = []\n",
    "\n",
    "for split in splits:\n",
    "    print(f\"\\nTraining with Validation Split: {int(split*100)}%\")\n",
    "    \n",
    "    loader = FashionMNISTLoader(batch_size=64, val_split=split) # Initialize the data loader with the current split\n",
    "\n",
    "    train_loader, val_loader, test_loader = loader.get_loaders() # Get the data loaders\n",
    "    print(f\"DataLoaders initialized with batch size 64 and val_split {split}\")\n",
    "\n",
    "    model = CNN() # Initialize the model\n",
    "\n",
    "    trainer = Trainer(model, train_loader, test_loader, val_loader, device=device) # Initialize the trainer\n",
    "\n",
    "    trainer.train(epochs=10) # Train the model\n",
    "    print(f\"Training completed for {int(split*100)}% validation split.\")\n",
    "\n",
    "    acc = trainer.evaluate() # Evaluate the model on the test set\n",
    "\n",
    "    print(f\"Test Accuracy with {int(split*100)}% val split: {acc:.4f}\")\n",
    "    results.append((split, acc))\n",
    "\n",
    "\n",
    "print(\"\\n=== Summary of Results ===\")\n",
    "for split, acc in results:\n",
    "    print(f\"Val Split: {int(split*100)}% | Test Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b35bc2e",
   "metadata": {},
   "source": [
    "# Task 3 : Learning Rate Experiments\n",
    "\n",
    "We tested the CNN performance using the best train/validation split ratio (10%) across different learning rates:  \n",
    "`[0.001, 0.01, 0.1, 1, 10]`.\n",
    "\n",
    "**Findings:**\n",
    "- Very small learning rates (0.001) led to slow convergence, underfitting the data.\n",
    "- Moderate rates (0.01 and 0.1) performed best.\n",
    "- Large rates (1 and 10) caused instability or divergence.\n",
    "\n",
    "| Learning Rate | Test Accuracy |\n",
    "|---------------|---------------|\n",
    "| 0.001         | 0.5643        |\n",
    "| 0.01          | 0.8504        |\n",
    "| 0.1           | 0.9033        |\n",
    "| 1             | 0.1000        |\n",
    "| 10            | 0.1000        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf97004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3\n",
    "print(\"\\n=== Running Task 3 Experiments ===\")\\\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Set device to GPU if available, else CPU\n",
    "\n",
    "best_val_split = 0.1 # Assuming the best validation split is 0.1 based on previous results\n",
    "learning_rates = [0.001, 0.01, 0.1, 1, 10] # Learning rates to test\n",
    "lr_results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nTraining with Learning Rate: {lr}\")\n",
    "    \n",
    "    loader = FashionMNISTLoader(batch_size=64, val_split=best_val_split) # Initialize the data loader with the best split\n",
    "\n",
    "    train_loader, val_loader, test_loader = loader.get_loaders() # Get the data loaders\n",
    "    print(f\"DataLoaders initialized with batch size 64 and val_split {best_val_split}\")\n",
    "\n",
    "\n",
    "    model = CNN() # Initialize the model\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr) # Initialize the optimizer with the current learning rate\n",
    "    trainer = Trainer(model, train_loader, test_loader, val_loader, optimizer, device=device) # Initialize the trainer\n",
    "\n",
    "    trainer.train(epochs=10) # Train the model\n",
    "    print(f\"Training completed for learning rate {lr}.\")\n",
    "\n",
    "    acc = trainer.evaluate() # Evaluate the model on the test set\n",
    "\n",
    "    print(f\"Test Accuracy with learning rate {lr}: {acc:.4f}\")\n",
    "    lr_results.append((lr, acc))\n",
    "\n",
    "print(\"\\n=== Summary of Learning Rate Results ===\")\n",
    "for lr, acc in lr_results:\n",
    "    print(f\"Learning Rate: {lr} | Test Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca77328",
   "metadata": {},
   "source": [
    "# Task 4: Optimizer Comparison – Adam vs SGD\n",
    "\n",
    "Using the best learning rate (`0.1`) and validation split (`20%`) from previous experiments, we replaced the SGD optimizer with the Adam optimizer and retrained the model using the same training setup.\n",
    "\n",
    "**Findings:**\n",
    "- Adam generally shows faster convergence and slightly better generalization compared to SGD across a range of learning rates.\n",
    "- Although, for this dataset and CNN architecture, Adam was performed significantly worse over SGD.\n",
    "- After research, Adam benifits from a more adaptive learning rate and having such a high rate a 0.1 to start off, must have been too aggressive to start off\n",
    "\n",
    "| Optimizer | Learning Rate | Test Accuracy |\n",
    "|----------|----------------|---------------|\n",
    "| SGD      | 0.1            | 0.9033        |\n",
    "| Adam     | 0.1            | 0.1000        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0527553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running Task 4 Experiments ===\n",
      "\n",
      "Training with Learning Rate: 0.1\n",
      "DataLoaders initialized with batch size 64 and val_split 0.1\n",
      "Epoch 1/10, Loss: 12.177495105995385\n",
      "Epoch 2/10, Loss: 1.770245125932151\n",
      "Epoch 3/10, Loss: 2.1743160456560235\n",
      "Epoch 4/10, Loss: 2.312556399553308\n",
      "Epoch 5/10, Loss: 2.3117938807225342\n",
      "Epoch 6/10, Loss: 2.3114005962819286\n",
      "Epoch 7/10, Loss: 2.312406127486749\n",
      "Epoch 8/10, Loss: 2.3118031894991184\n",
      "Epoch 9/10, Loss: 2.311489098727421\n",
      "Epoch 10/10, Loss: 2.3124969590331705\n",
      "Test Accuracy with Adam optimizer: 0.1000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Running Task 4 Experiments ===\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "best_val_split = 0.1 # Assuming the best validation split is 0.1 based on previous results\n",
    "best_learning_rate = 0.1 # Assuming the best learning rate is 0.1 based on previous results\n",
    "\n",
    "print(f\"\\nTraining with Learning Rate: {best_learning_rate}\")\n",
    "loader = FashionMNISTLoader(batch_size=64, val_split=best_val_split)\n",
    "\n",
    "train_loader, val_loader, test_loader = loader.get_loaders() \n",
    "print(f\"DataLoaders initialized with batch size 64 and val_split {best_val_split}\")\n",
    "\n",
    "model = CNN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_learning_rate) # Initialize the adam optimizer \n",
    "trainer = Trainer(model, train_loader, test_loader, val_loader, optimizer, device=device)\n",
    "\n",
    "trainer.train(epochs=10)\n",
    "\n",
    "adam_accuracy = trainer.evaluate()\n",
    "print(f\"Test Accuracy with Adam optimizer: {adam_accuracy:.4f}\")\n",
    "\n",
    "###################################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c69354",
   "metadata": {},
   "source": [
    "# Task 5: One-Class AUC Evaluation\n",
    "\n",
    "We evaluated the best-performing CNN model in a one-vs-all setting using label `2` (Pullover) as the positive class, and all other labels as the negative class. We calculated the Area Under the ROC Curve (AUC) as our evaluation metric.\n",
    "\n",
    "This binary setup helps assess how well the model separates one specific class from others, beyond multi-class accuracy.\n",
    "\n",
    "**Result:**\n",
    "- AUC Score for Label 2: `0.9910`\n",
    "\n",
    "An AUC near 1.0 indicates excellent separation; values closer to 0.5 suggest poor class distinction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d445f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running Task 5: One-vs-All AUC Evaluation ===\n",
      "DataLoaders initialized with batch size 64 and val_split 0.1 and learning rate 0.1\n",
      "Epoch 1/10, Loss: 0.8410891521527869\n",
      "Epoch 2/10, Loss: 0.3872901620210912\n",
      "Epoch 3/10, Loss: 0.3127447700666449\n",
      "Epoch 4/10, Loss: 0.2740465437825681\n",
      "Epoch 5/10, Loss: 0.24535594770234626\n",
      "Epoch 6/10, Loss: 0.22276802713678207\n",
      "Epoch 7/10, Loss: 0.20679125978977758\n",
      "Epoch 8/10, Loss: 0.18940740286580052\n",
      "Epoch 9/10, Loss: 0.17740572119016923\n",
      "Epoch 10/10, Loss: 0.16434979045507608\n",
      "AUC Score (class 2 vs all): 0.9901\n"
     ]
    }
   ],
   "source": [
    "# Task 5: AUC Evaluation for Class 2 vs All\n",
    "print(\"\\n=== Running Task 5: One-vs-All AUC Evaluation ===\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "best_val_split = 0.1\n",
    "best_learning_rate = 0.1\n",
    "\n",
    "# Load data\n",
    "loader = FashionMNISTLoader(batch_size=64, val_split=best_val_split)\n",
    "train_loader, val_loader, test_loader = loader.get_loaders()\n",
    "print(f\"DataLoaders initialized with batch size 64 and val_split {best_val_split} and learning rate {best_learning_rate}\")\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = CNN()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=best_learning_rate)\n",
    "trainer = Trainer(model, train_loader, test_loader, val_loader, optimizer=optimizer, device=device)\n",
    "\n",
    "# Train model\n",
    "trainer.train(epochs=10)\n",
    "\n",
    "# Evaluate AUC for class 2 (positive) vs all (negative)\n",
    "auc_score = trainer.evaluate_auc(pos_class=2)\n",
    "print(f\"AUC Score (class 2 vs all): {auc_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
